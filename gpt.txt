Исправь эту программу так, чтобы она:
1) Обучалась на тренировочных данных, а далее ее можно было применять так:
1.1) Подавались валидационные данные, и показывалась точность распознавания на валидационных данных
1.2) А потом можно было подать тестовые данные, и если вероятность принадлежности к классу 1 больше 0.7, то выводилась надпись "Это газ nh3", иначе выводилась надпись "Газ NO2" 
```
# Данные
X_train = np.array(X_train_nh3 + X_train_no2_2rh)
print (f"Длина X_train: {len(X_train)}")
y_train = np.array(target_nh3 + target_no2_2rh)
print (f"Длина y_train: {len(y_train)}")

X_val = X_val_nh3 + X_val_no2_2rh
print(f"Длина X_val: {len(X_val)}")
y_val = target_nh3_val + target_no2_2rh_val
print(f"Длина y_val: {len(y_val)}")

# Преобразование данных в тензоры PyTorch
x_train_tensor = torch.tensor(X_train, dtype=torch.float32)# из нумпая в тензор, данные как float32 
y_train_tensor = torch.tensor(y_train, dtype=torch.long) #long -  для классификации(указывает типа, что это прям переменные класса именно, а не просто признаки)
x_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.long)

# Модель RNN
class SimpleRNN(nn.Module):#базовый модуль для всех нейронок библиотеки
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleRNN, self).__init__()# конструктор из класса-родителя, для инициализации
        self.hidden_size = hidden_size
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)#dim батча - первая размерность входного тензора
        self.fc = nn.Linear(hidden_size, output_size)#полностью связанный слой, используется для преобразования скрытого состояния в выходной вектор
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        #Создаем тензор нулей, начальное скрытое состояние (h0)
        #1: dim, соответствующая направлению RNN (в данном случае - 1, так как RNN не двунаправленный)
        #x.size(0): Размер батча (количество примеров в батче)
        #self.hidden_size: Размер скрытого состояния
        h0 = torch.zeros(1, x.size(0), self.hidden_size)# Инициализация hidden state, важное изменение для batch_first=True
        out, hn = self.rnn(x, h0) #hn - последнее скрытое состояние, out - выход rnn содержит скрытое состояние на каждом временном шаге
        out = self.fc(out[:, -1, :]) #все элементы батча, посл вр шаг, все знач скрыт. сост.
        out = self.sigmoid(out)
        return out

# Гиперпараметры
input_size = 6# количество элементов в каждом подмассиве нашего массива
hidden_size = 4# подбираем сами, такое количество вполне норм сейчас
output_size = 1# выходной слой
learning_rate = 0.01# размер шага для нашего алгоритма оптимизации
epochs = 22
batch_size = len(X_train)#соответствует длине данных, В ОБЩЕМ СЛУЧАЕ ЛУЧШЕ ЧУТЬ ПОДПРАВИТЬ ЭТО


model = SimpleRNN(input_size, hidden_size, output_size)
criterion = nn.BCELoss() # Бинарная кросс-энтропия
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

#для остановки, если ошибка не уменьшается
best_val_accuracy = 0
best_epoch = 0

val_batch_size = len(X_val)
# Обучение
for epoch in range(epochs):
    optimizer.zero_grad()# делаем нулевой градиент, иначе лажа с весами будет
    outputs = model(x_train_tensor.unsqueeze(1))#добавляем еще один параметр время
    loss = criterion(outputs.squeeze(1), y_train_tensor.float())
    loss.backward()
    optimizer.step()# шагаем
    with torch.no_grad():
            # Оценка на валидационных данных с использованием батчей
            val_predictions = []
            val_outputs = []
            for i in range(0, len(x_val_tensor), val_batch_size):
                val_batch = x_val_tensor[i:i + val_batch_size].unsqueeze(1)
                batch_outputs = model(val_batch)
                val_outputs.extend(batch_outputs.squeeze(1).tolist())
                batch_predictions = (batch_outputs.squeeze(1) > 0.5).float()
                val_predictions.extend(batch_predictions.tolist())

            val_predictions = np.array(val_predictions)
            val_outputs = np.array(val_outputs)
            val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions)
            val_precision = precision_score(y_val_tensor.numpy(), val_predictions)
            val_recall = recall_score(y_val_tensor.numpy(), val_predictions)
            val_f1 = f1_score(y_val_tensor.numpy(), val_predictions)


            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, '
                f'Val Acc: {val_accuracy:.4f}, Val Prec: {val_precision:.4f}, Val Rec: {val_recall:.4f}, Val F1: {val_f1:.4f}')



    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        best_epoch = epoch + 1
        #torch.save(model.state_dict(), 'best_model.pth') # Сохранение лучшей модели (опционально)

        print(f'\nBest Validation Accuracy: {best_val_accuracy:.4f} at epoch {best_epoch}')

#Загрузка лучшей модели (опционально)
#model.load_state_dict(torch.load('best_model.pth'))


#Оценка точности на тренировочных данных и валидационных данных после обучения
with torch.no_grad():
    train_outputs = model(x_train_tensor.unsqueeze(1))
    train_predictions = (train_outputs.squeeze(1) > 0.5).float()
    train_accuracy = accuracy_score(y_train_tensor.numpy(), train_predictions.numpy())
    print(f'\nTrain Accuracy: {train_accuracy:.4f}')


    val_outputs = model(x_val_tensor.unsqueeze(1))
    val_predictions = (val_outputs.squeeze(1) > 0.5).float()
    val_accuracy = accuracy_score(y_val_tensor.numpy(), val_predictions.numpy())
    print(f'\nValidation Accuracy: {val_accuracy:.4f}')
```
